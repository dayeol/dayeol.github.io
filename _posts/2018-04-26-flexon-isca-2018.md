---
layout: post
author: Dayeol Lee
title: "Flexon: A Flexible Digital Neuron for Efficient Spiking Neural Network Simulations"
---

# What is it about?

The brain consists of billions of neurons, which have been of special interest to
neuroscientists due to their ability to communicate with others using electrochemical signals called *spikes*.
They are believed to be the generator of various functions of the brain.
Although there is still no consensus about how the brain encodes information with the spikes,
it is evident that the brain employs temporal information such as spike timing or frequency.

*[Spiking neural network](https://en.wi/kipedia.org/wiki/Spiking_neural_network)* 
has been used to simulate the brain.
SNNs incorporate the concept of *time* into neurons by modeling each neuron 
with time-varying state variables associated with several differential equations.

In this project, I analyzed common primitives of existing neuron models and
designed an ASIC accelerator that can efficiently simulate a wide range of neuron models by 
exploiting those primitives.
The accelerator is highly flexible than the other ASIC accelerators, and also enables
orders-of-magnitude efficient simulation than the general purpose processors.



# Publication
This work has been published in <a href="http://iscaconf.org/isca2018/">ISCA 2018</a>.

<a href="{{site.url}}/publications/flexon-isca18.pdf">{% octicon file-pdf %} <b>Flexon: A Flexible Digital Neuron for Efficient Spiking Neural Network Simulations</b></a>

<a href="http://iscaconf.org/isca2018/slides/4A1.pdf">{% octicon file-pdf %} <b>ISCA'18 Presentation
Slides</b></a>

<b>{% octicon device-camera-video %} Lightening Talk: </b> Gwangmu, a co-author of the paper, gave a nice lightening talk. 
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/_ICSrT9sNzI" frameborder="0"
allow="autoplay; encrypted-media" allowfullscreen></iframe>
</center>
**Abstract** - Spiking Neural Networks (SNNs) play an important role in neuroscience as they help
neuroscientists understand how the nervous system works. To model the nervous system, SNNs
incorporate the concept of time into neurons and inter-neuron interactions called spikes; a neuron's
internal state changes with respect to time and input spikes, and a neuron fires an output spike
when its internal state satisfies certain conditions. As the neurons forming the nervous system
behave differently, SNN simulation frameworks must be able to simulate the diverse behaviors of the
neurons. To support any neuron models, some frameworks rely on general purpose processors at the
cost of inefficiency in simulation speed and energy consumption. The other frameworks employ
specialized accelerators to overcome the inefficiency; however, the accelerators support only a
limited set of neuron models due to their model-driven designs, making accelerator-based frameworks
unable to simulate target SNNs.

In this paper, we present Flexon, a flexible digital neuron which
exploits the biologically common features shared by diverse neuron models, to enable efficient SNN
simulations. To design Flexon, we first collect SNNs from prior work in neuroscience research and
analyze the neuron models the SNNs employ. From the analysis, we observe that the neuron models
share a set of biologically common features, and that the features can be combined to simulate a
significantly larger set of neuron behaviors than the existing model-driven designs. Furthermore, we
find that the features share a small set of computational primitives which can be exploited to
further reduce the chip area. The resulting digital neurons, Flexon and spatially folded Flexon, are
flexible, highly efficient, and can be easily integrated with existing hardware. Our prototyping
results using TSMC 45 nm standard cell library show that a 12-neuron Flexon array improves energyefficiency by 6,186x and 422x over CPU and GPU, respectively, in a small footprint of 9.26 mm2. The
results also show that a 72-neuron spatially folded Flexon array incurs a smaller footprint of 7.62
mm2 and achieves geomean speedups of 122.45x and 9.83x over CPU and GPU, respectively.
